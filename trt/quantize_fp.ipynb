{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "416ad84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47b1233",
   "metadata": {},
   "source": [
    "## Inference with naive saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c46c485d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yidi/venv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: resnet50_saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(weights='imagenet')\n",
    "# Save the entire model as a SavedModel.\n",
    "model.save('resnet50_saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "284b4bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "./data/img0.JPG - Predicted: [('n02110185', 'Siberian_husky', 0.5568136), ('n02109961', 'Eskimo_dog', 0.41662624), ('n02110063', 'malamute', 0.021314148)]\n"
     ]
    }
   ],
   "source": [
    "# Inference with naive model\n",
    "model = tf.keras.models.load_model('resnet50_saved_model')\n",
    "img_path = './data/img0.JPG'  # Siberian_husky\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('{} - Predicted: {}'.format(img_path, decode_predictions(preds, top=3)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d378bc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batched_input shape:  (8, 224, 224, 3)\n",
      "Step 0: 29.0ms\n",
      "Step 50: 29.2ms\n",
      "Step 100: 29.5ms\n",
      "Step 150: 29.2ms\n",
      "Step 200: 29.2ms\n",
      "Step 250: 29.5ms\n",
      "Step 300: 30.2ms\n",
      "Step 350: 30.5ms\n",
      "Step 400: 30.4ms\n",
      "Step 450: 30.1ms\n",
      "Step 500: 30.1ms\n",
      "Step 550: 30.1ms\n",
      "Step 600: 30.1ms\n",
      "Step 650: 29.6ms\n",
      "Step 700: 29.8ms\n",
      "Step 750: 29.7ms\n",
      "Step 800: 29.7ms\n",
      "Step 850: 29.3ms\n",
      "Step 900: 29.6ms\n",
      "Step 950: 37.3ms\n",
      "Throughput: 265 images/s\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "batched_input = np.zeros((batch_size, 224, 224, 3), dtype=np.float32)\n",
    "\n",
    "for i in range(batch_size):\n",
    "  img_path = './data/img%d.JPG' % (i % 4)\n",
    "  img = image.load_img(img_path, target_size=(224, 224))\n",
    "  x = image.img_to_array(img)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  x = preprocess_input(x)\n",
    "  batched_input[i, :] = x\n",
    "batched_input = tf.constant(batched_input)\n",
    "print('batched_input shape: ', batched_input.shape)\n",
    "\n",
    "# Benchmarking throughput\n",
    "N_warmup_run = 50\n",
    "N_run = 1000\n",
    "elapsed_time = []\n",
    "\n",
    "for i in range(N_warmup_run):\n",
    "  preds = model.predict(batched_input)\n",
    "\n",
    "for i in range(N_run):\n",
    "  start_time = time.time()\n",
    "  preds = model.predict(batched_input)\n",
    "  end_time = time.time()\n",
    "  elapsed_time = np.append(elapsed_time, end_time - start_time)\n",
    "  if i % 50 == 0:\n",
    "    print('Step {}: {:4.1f}ms'.format(i, (elapsed_time[-50:].mean()) * 1000))\n",
    "\n",
    "print('Throughput: {:.0f} images/s'.format(N_run * batch_size / elapsed_time.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa4eb1",
   "metadata": {},
   "source": [
    "## TRT-FP32 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b65945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to TF-TRT FP32...\n",
      "INFO:tensorflow:Linked TensorRT version: (6, 0, 1)\n",
      "INFO:tensorflow:Loaded TensorRT version: (6, 0, 1)\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: resnet50_saved_model_TFTRT_FP32/assets\n",
      "Done Converting to TF-TRT FP32\n"
     ]
    }
   ],
   "source": [
    "# TF-TRT FP32 Model\n",
    "print('Converting to TF-TRT FP32...')\n",
    "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.FP32,\n",
    "                                                               max_workspace_size_bytes=8000000000)\n",
    "converter = trt.TrtGraphConverterV2(input_saved_model_dir='resnet50_saved_model',\n",
    "                                    conversion_params=conversion_params)\n",
    "converter.convert()\n",
    "converter.save(output_saved_model_dir='resnet50_saved_model_TFTRT_FP32')\n",
    "print('Done Converting to TF-TRT FP32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21195c1",
   "metadata": {},
   "source": [
    "## TRT-FP16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5314fa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to TF-TRT FP16...\n",
      "INFO:tensorflow:Linked TensorRT version: (6, 0, 1)\n",
      "INFO:tensorflow:Loaded TensorRT version: (6, 0, 1)\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "WARNING:tensorflow:From /home/yidi/venv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: resnet50_saved_model_TFTRT_FP16/assets\n",
      "Done Converting to TF-TRT FP16\n"
     ]
    }
   ],
   "source": [
    "print('Converting to TF-TRT FP16...')\n",
    "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
    "    precision_mode=trt.TrtPrecisionMode.FP16,\n",
    "    max_workspace_size_bytes=8000000000)\n",
    "converter = trt.TrtGraphConverterV2(\n",
    "   input_saved_model_dir='resnet50_saved_model', conversion_params=conversion_params)\n",
    "converter.convert()\n",
    "converter.save(output_saved_model_dir='resnet50_saved_model_TFTRT_FP16')\n",
    "print('Done Converting to TF-TRT FP16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba909de",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ba192e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batched_input shape:  (8, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "batched_input = np.zeros((batch_size, 224, 224, 3), dtype=np.float32)\n",
    "\n",
    "for i in range(batch_size):\n",
    "  img_path = './data/img%d.JPG' % (i % 4)\n",
    "  img = image.load_img(img_path, target_size=(224, 224))\n",
    "  x = image.img_to_array(img)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  x = preprocess_input(x)\n",
    "  batched_input[i, :] = x\n",
    "batched_input = tf.constant(batched_input)\n",
    "print('batched_input shape: ', batched_input.shape)\n",
    "\n",
    "def benchmark_tftrt(input_saved_model):\n",
    "    saved_model_loaded = tf.saved_model.load(input_saved_model, tags=[tag_constants.SERVING])\n",
    "    infer = saved_model_loaded.signatures['serving_default']\n",
    "\n",
    "    N_warmup_run = 50\n",
    "    N_run = 1000\n",
    "    elapsed_time = []\n",
    "\n",
    "    for i in range(N_warmup_run):\n",
    "      labeling = infer(batched_input)\n",
    "\n",
    "    for i in range(N_run):\n",
    "      start_time = time.time()\n",
    "      labeling = infer(batched_input)\n",
    "      #prob = labeling['probs'].numpy()\n",
    "      end_time = time.time()\n",
    "      elapsed_time = np.append(elapsed_time, end_time - start_time)\n",
    "      if i % 50 == 0:\n",
    "        print('Step {}: {:4.1f}ms'.format(i, (elapsed_time[-50:].mean()) * 1000))\n",
    "\n",
    "    print('Throughput: {:.0f} images/s'.format(N_run * batch_size / elapsed_time.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07aaf05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:  6.8ms\n",
      "Step 50:  6.8ms\n",
      "Step 100:  6.8ms\n",
      "Step 150:  6.8ms\n",
      "Step 200:  6.8ms\n",
      "Step 250:  6.8ms\n",
      "Step 300:  6.8ms\n",
      "Step 350:  6.8ms\n",
      "Step 400:  6.9ms\n",
      "Step 450:  6.9ms\n",
      "Step 500:  6.8ms\n",
      "Step 550:  6.8ms\n",
      "Step 600:  6.9ms\n",
      "Step 650:  6.9ms\n",
      "Step 700:  6.9ms\n",
      "Step 750:  6.9ms\n",
      "Step 800:  6.8ms\n",
      "Step 850:  6.9ms\n",
      "Step 900:  6.9ms\n",
      "Step 950:  6.9ms\n",
      "Throughput: 1168 images/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_tftrt('resnet50_saved_model_TFTRT_FP32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12567c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:  6.8ms\n",
      "Step 50:  6.8ms\n",
      "Step 100:  6.9ms\n",
      "Step 150:  6.9ms\n",
      "Step 200:  6.9ms\n",
      "Step 250:  6.9ms\n",
      "Step 300:  6.9ms\n",
      "Step 350:  6.9ms\n",
      "Step 400:  6.9ms\n",
      "Step 450:  6.9ms\n",
      "Step 500:  6.9ms\n",
      "Step 550:  6.9ms\n",
      "Step 600:  6.9ms\n",
      "Step 650:  6.9ms\n",
      "Step 700:  6.9ms\n",
      "Step 750:  6.9ms\n",
      "Step 800:  6.9ms\n",
      "Step 850:  6.9ms\n",
      "Step 900:  6.9ms\n",
      "Step 950:  6.9ms\n",
      "Throughput: 1163 images/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_tftrt('resnet50_saved_model_TFTRT_FP16')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
